{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node Embedding\n",
    "\n",
    "## Use doc2vec to find the node in the Ontology graph from the protein function text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.6/site-packages (3.5)\r\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/site-packages (from nltk) (7.1.2)\r\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/site-packages (from nltk) (0.17.0)\r\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.6/site-packages (from nltk) (2020.11.13)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/site-packages (from nltk) (4.54.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hbagheri/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/hbagheri/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the dependencies\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopset = set(stopwords.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One\n",
    "Put name and synonyms together as a document. Find the most similar one with doc2vec\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRO dataset download here:\n",
    "- https://bioportal.bioontology.org/ontologies/PR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321481\n",
      "321481\n"
     ]
    }
   ],
   "source": [
    "with open(\"../gitdata/pro_reasoned.obo\") as fp:\n",
    "    tags = []\n",
    "    data = []\n",
    "    each_tag = \"\"\n",
    "    each_term = \"\"\n",
    "    id2name = {}\n",
    "    id2def = {}\n",
    "    for line in fp.readlines():\n",
    "        if \"[Term]\" in line:\n",
    "            if len(each_tag) > 0:\n",
    "                tags.append(each_tag.strip(\" \").strip(\"\\n\"))\n",
    "                data.append(each_term.strip(\" \".strip(\"\\n\")))\n",
    "                id2name[each_tag.strip(\" \").strip(\"\\n\")] = name\n",
    "                id2synonym[each_tag.strip(\" \").strip(\"\\n\")] = defi\n",
    "            each_tag = \"\"\n",
    "            each_term = \"\"\n",
    "        if \"id:\" in line[0:3]:\n",
    "            each_tag += (\" \" + line.replace(\"id: \", \"\"))\n",
    "        if \"name:\" in line[0:5]:\n",
    "            name = line.replace(\"name: \", \"\").strip(\"\\n\").strip(\" \")\n",
    "            each_term += (\" \" + line.replace(\"name: \", \"\").strip(\"\\n\"))\n",
    "        if \"def:\" in line[0:4]:\n",
    "            defi = line.replace(\"def: \", \"\").strip(\"\\n\").strip(\" \")\n",
    "            each_term += (\" \" + line.replace(\"def: \", \"\").strip(\"\\n\"))\n",
    "#         if \"synonym:\" in line[0:10]:\n",
    "#             synonym = line.replace(\"synonym: \", \"\").strip(\"\\n\").strip(\" \")\n",
    "#             each_term += (\" \" + line.replace(\"synonym: \", \"\").strip(\"\\n\"))\n",
    "print(len(tags))\n",
    "print(len(data))\n",
    "# print(id2name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by Hamid: this is the backup code I modified\n",
    "with open(\"../gitdata/pro_reasoned.obo\") as fp:\n",
    "    tags = []\n",
    "    data = []\n",
    "    each_tag = \"\"\n",
    "    each_term = \"\"\n",
    "    id2name = {}\n",
    "    id2synonym = {}\n",
    "    for line in fp.readlines():\n",
    "        if \"[Term]\" in line:\n",
    "            if len(each_tag) > 0:\n",
    "                tags.append(each_tag.strip(\" \").strip(\"\\n\"))\n",
    "                data.append(each_term.strip(\" \".strip(\"\\n\")))\n",
    "                id2name[each_tag.strip(\" \").strip(\"\\n\")] = name\n",
    "                id2synonym[each_tag.strip(\" \").strip(\"\\n\")] = synonym\n",
    "            each_tag = \"\"\n",
    "            each_term = \"\"\n",
    "        if \"id:\" in line[0:3]:\n",
    "            each_tag += (\" \" + line.replace(\"id: \", \"\"))\n",
    "        if \"name:\" in line[0:5]:\n",
    "            name = line.replace(\"name: \", \"\").strip(\"\\n\").strip(\" \")\n",
    "            each_term += (\" \" + line.replace(\"name: \", \"\").strip(\"\\n\"))\n",
    "#         if \"def:\" in line[0:4]:\n",
    "#             each_term += (\" \" + line.replace(\"def: \", \"\").strip(\"\\n\"))\n",
    "        if \"synonym:\" in line[0:10]:\n",
    "            synonym = line.replace(\"synonym: \", \"\").strip(\"\\n\").strip(\" \")\n",
    "            each_term += (\" \" + line.replace(\"synonym: \", \"\").strip(\"\\n\"))\n",
    "print(len(tags))\n",
    "print(len(data))\n",
    "# print(id2name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two\n",
    "\n",
    "Regard name and each synonym as a document separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "910313\n",
      "CPU times: user 2.65 s, sys: 168 ms, total: 2.82 s\n",
      "Wall time: 2.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open(\"../data/pro_reasoned.obo\") as fp:\n",
    "    tags = []\n",
    "    data = []\n",
    "    each_tag = \"\"\n",
    "    each_term = \"\"\n",
    "    id2name = {}\n",
    "    id2synonym = {}\n",
    "    for line in fp.readlines():\n",
    "        if \"[Term]\" in line:\n",
    "            if len(each_tag) > 0:\n",
    "                tags.append(each_tag.strip(\" \").strip(\"\\n\"))\n",
    "                data.append(each_term.strip(\" \".strip(\"\\n\")))\n",
    "                id2name[each_tag.strip(\" \").strip(\"\\n\")] = name\n",
    "#                 id2synonym[each_tag.strip(\" \").strip(\"\\n\")] = synonym\n",
    "            each_tag = \"\"\n",
    "            each_term = \"\"\n",
    "        if \"id:\" in line[0:3]:\n",
    "            each_tag += (\" \" + line.replace(\"id: \", \"\"))\n",
    "        if \"name:\" in line[0:5]:\n",
    "            name = line.replace(\"name: \", \"\").strip(\"\\n\").strip(\" \")\n",
    "            each_term += (\" \" + line.replace(\"name: \", \"\").strip(\"\\n\"))\n",
    "#         if \"def:\" in line[0:4]:\n",
    "#             each_term += (\" \" + line.replace(\"def: \", \"\").strip(\"\\n\"))\n",
    "        if \"synonym:\" in line[0:10]:\n",
    "            synonym = line.replace(\"synonym: \", \"\").strip(\"\\n\").strip(\" \")\n",
    "#             each_term += (\" \" + line.replace(\"synonym: \", \"\").strip(\"\\n\"))\n",
    "            data.append(synonym.strip(\" \").strip(\"\\n\"))\n",
    "            tags.append(each_tag.strip(\" \").strip(\"\\n\"))\n",
    "            each_tag = each_tag.strip(\" \").strip(\"\\n\")\n",
    "            if each_tag not in id2synonym:\n",
    "                id2synonym[each_tag] = []\n",
    "            id2synonym[each_tag].append(synonym)\n",
    "print(len(tags))\n",
    "# print(data)\n",
    "# print(id2synonym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 38s, sys: 1.89 s, total: 2min 40s\n",
      "Wall time: 2min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tag_dict = {}\n",
    "for i in range(len(tags)):\n",
    "    tag_dict[i] = tags[i]\n",
    "tagged_data = [TaggedDocument(words=[token for token in word_tokenize(_d.lower()) if token not in stopset and len(token) > 1], \n",
    "                              tags=[tags[i]]) for i, _d in enumerate(data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in tagged_data:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/gensim/models/doc2vec.py:319: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n",
      "iteration 11\n",
      "iteration 12\n",
      "iteration 13\n",
      "iteration 14\n",
      "iteration 15\n",
      "iteration 16\n",
      "iteration 17\n",
      "iteration 18\n",
      "iteration 19\n",
      "iteration 20\n",
      "iteration 21\n",
      "iteration 22\n",
      "iteration 23\n",
      "iteration 24\n",
      "iteration 25\n",
      "iteration 26\n",
      "iteration 27\n",
      "iteration 28\n",
      "iteration 29\n",
      "iteration 30\n",
      "iteration 31\n",
      "iteration 32\n",
      "iteration 33\n",
      "iteration 34\n",
      "iteration 35\n",
      "iteration 36\n",
      "iteration 37\n",
      "iteration 38\n",
      "iteration 39\n",
      "iteration 40\n",
      "iteration 41\n",
      "iteration 42\n",
      "iteration 43\n",
      "iteration 44\n",
      "iteration 45\n",
      "iteration 46\n",
      "iteration 47\n",
      "iteration 48\n",
      "iteration 49\n",
      "iteration 50\n",
      "iteration 51\n",
      "iteration 52\n",
      "iteration 53\n",
      "iteration 54\n",
      "iteration 55\n",
      "iteration 56\n",
      "iteration 57\n",
      "iteration 58\n",
      "iteration 59\n",
      "iteration 60\n",
      "iteration 61\n",
      "iteration 62\n",
      "iteration 63\n",
      "iteration 64\n",
      "iteration 65\n",
      "iteration 66\n",
      "iteration 67\n",
      "iteration 68\n",
      "iteration 69\n",
      "iteration 70\n",
      "iteration 71\n",
      "iteration 72\n",
      "iteration 73\n",
      "iteration 74\n",
      "iteration 75\n",
      "iteration 76\n",
      "iteration 77\n",
      "iteration 78\n",
      "iteration 79\n",
      "iteration 80\n",
      "iteration 81\n",
      "iteration 82\n",
      "iteration 83\n",
      "iteration 84\n",
      "iteration 85\n",
      "iteration 86\n",
      "iteration 87\n",
      "iteration 88\n",
      "iteration 89\n",
      "iteration 90\n",
      "iteration 91\n",
      "iteration 92\n",
      "iteration 93\n",
      "iteration 94\n",
      "iteration 95\n",
      "iteration 96\n",
      "iteration 97\n",
      "iteration 98\n",
      "iteration 99\n",
      "iteration 100\n",
      "iteration 101\n",
      "iteration 102\n",
      "iteration 103\n",
      "iteration 104\n",
      "iteration 105\n",
      "iteration 106\n",
      "iteration 107\n",
      "iteration 108\n",
      "iteration 109\n",
      "iteration 110\n",
      "iteration 111\n",
      "iteration 112\n",
      "iteration 113\n",
      "iteration 114\n",
      "iteration 115\n",
      "iteration 116\n",
      "iteration 117\n",
      "iteration 118\n",
      "iteration 119\n",
      "iteration 120\n",
      "iteration 121\n",
      "iteration 122\n",
      "iteration 123\n",
      "iteration 124\n",
      "iteration 125\n",
      "iteration 126\n",
      "iteration 127\n",
      "iteration 128\n",
      "iteration 129\n",
      "iteration 130\n",
      "iteration 131\n",
      "iteration 132\n",
      "iteration 133\n",
      "iteration 134\n",
      "iteration 135\n",
      "iteration 136\n",
      "iteration 137\n",
      "iteration 138\n",
      "iteration 139\n",
      "iteration 140\n",
      "iteration 141\n",
      "iteration 142\n",
      "iteration 143\n",
      "iteration 144\n",
      "iteration 145\n",
      "iteration 146\n",
      "iteration 147\n",
      "iteration 148\n",
      "iteration 149\n",
      "iteration 150\n",
      "iteration 151\n",
      "iteration 152\n",
      "iteration 153\n",
      "iteration 154\n",
      "iteration 155\n",
      "iteration 156\n",
      "iteration 157\n",
      "iteration 158\n",
      "iteration 159\n",
      "iteration 160\n",
      "iteration 161\n",
      "iteration 162\n",
      "iteration 163\n",
      "iteration 164\n",
      "iteration 165\n",
      "iteration 166\n",
      "iteration 167\n",
      "iteration 168\n",
      "iteration 169\n",
      "iteration 170\n",
      "iteration 171\n",
      "iteration 172\n",
      "iteration 173\n",
      "iteration 174\n",
      "iteration 175\n",
      "iteration 176\n",
      "iteration 177\n",
      "iteration 178\n",
      "iteration 179\n",
      "iteration 180\n",
      "iteration 181\n",
      "iteration 182\n",
      "iteration 183\n",
      "iteration 184\n",
      "iteration 185\n",
      "iteration 186\n",
      "iteration 187\n",
      "iteration 188\n",
      "iteration 189\n",
      "iteration 190\n",
      "iteration 191\n",
      "iteration 192\n",
      "iteration 193\n",
      "iteration 194\n",
      "iteration 195\n",
      "iteration 196\n",
      "iteration 197\n",
      "iteration 198\n",
      "iteration 199\n",
      "Model Saved\n",
      "CPU times: user 4h 53min 45s, sys: 38min 45s, total: 5h 32min 30s\n",
      "Wall time: 7h 41min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "max_epochs = 200\n",
    "vec_size = 64\n",
    "alpha = 0.025\n",
    "\n",
    "model = Doc2Vec(size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.001,\n",
    "                min_count=2,\n",
    "                dm =0)\n",
    "  \n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print('iteration {0}'.format(epoch))\n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.iter)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.00002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "model.save(\"../gitdata/pro_split_name_synonym_d2v.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuepei/anaconda3/envs/labNR@3.6/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 609 ms, sys: 51.3 ms, total: 660 ms\n",
      "Wall time: 659 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "model= Doc2Vec.load(\"../gitdata/pro_split_name_synonym_d2v.model\")\n",
    "\n",
    "# #to find the vector of a document which is not in training data\n",
    "# test_data = \"telomere maintenance via recombination\"\n",
    "# test_data = [token for token in word_tokenize(test_data.lower()) if token not in stopset and len(token) > 1]\n",
    "# print(test_data)\n",
    "\n",
    "# v1 = model.infer_vector(test_data)\n",
    "# print(\"V1_infer\", v1)\n",
    "\n",
    "# model.docvecs.most_similar([v1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert protein name to protein id with name2id dictionary\n",
    "\n",
    "### Load data\n",
    "\n",
    "nr_tiny is the top 1000 lines of nr file. We use it as a test of our methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183 items loaded. \n",
      "CPU times: user 1.73 s, sys: 397 ms, total: 2.13 s\n",
      "Wall time: 4.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open(\"../gitdata/nr_tiny\") as fp:\n",
    "    protein_dataset = []\n",
    "    for line in fp.readlines():\n",
    "        if len(line) == 0 or \">\" != line[0]:\n",
    "            continue\n",
    "        line_key = line.split(\" \")[0]\n",
    "        aLine = line.strip(\"\").strip(\"\\n\").split(\"\\x01\")\n",
    "        name_list = [line_key]\n",
    "        for each_desc in aLine:\n",
    "            # e.g. of each_desc: \n",
    "            # NP_268346.1 30S ribosomal protein S18 [Lactococcus lactis subsp. lactis Il1403]\n",
    "            #  ** function name * \n",
    "            words = each_desc.split(\" \")\n",
    "            \n",
    "            # name is protein name\n",
    "            # find the first '[' as the end of name\n",
    "            indexs = [words.index(i) for i in words if \"[\" in i]\n",
    "            if len(indexs) > 0:\n",
    "                index = indexs[0]\n",
    "                name = \" \".join(words[1:index])\n",
    "            else:\n",
    "                name = \" \".join(words[1:])\n",
    "            \n",
    "            # pre-processing\n",
    "            name = name.replace(\",\", \"\")\n",
    "            name = name.replace(\".\", \"\")\n",
    "            name = name.lower()\n",
    "            if \"chain\" in name and len(name) > 40:\n",
    "                continue\n",
    "#             if \"hypothetical protein\" in name:\n",
    "#                 continue\n",
    "            name_list.append(name)  \n",
    "        protein_dataset.append(name_list)\n",
    "    print(\"{} items loaded. \".format(len(protein_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting goatools\n",
      "  Downloading goatools-1.0.14.tar.gz (15.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.1 MB 641 kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/site-packages (from goatools) (1.15.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/site-packages (from goatools) (1.5.4)\n",
      "Collecting docopt\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "Collecting nose\n",
      "  Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
      "\u001b[K     |████████████████████████████████| 154 kB 2.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pandas\n",
      "  Downloading pandas-1.1.5-cp36-cp36m-macosx_10_9_x86_64.whl (10.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.2 MB 2.2 MB/s eta 0:00:01    |███████████▉                    | 3.8 MB 777 kB/s eta 0:00:09\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/site-packages (from pandas->goatools) (2018.7)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/site-packages (from goatools) (1.15.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/site-packages (from pandas->goatools) (2.8.0)\n",
      "Collecting pydot\n",
      "  Downloading pydot-1.4.1-py2.py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.6/site-packages (from pydot->goatools) (2.3.0)\n",
      "Collecting pytest\n",
      "  Downloading pytest-6.1.2-py3-none-any.whl (272 kB)\n",
      "\u001b[K     |████████████████████████████████| 272 kB 294 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/site-packages (from pytest->goatools) (19.1.0)\n",
      "Collecting importlib-metadata>=0.12\n",
      "  Downloading importlib_metadata-3.1.1-py3-none-any.whl (9.6 kB)\n",
      "Collecting iniconfig\n",
      "  Downloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\n",
      "Collecting packaging\n",
      "  Downloading packaging-20.7-py2.py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.6/site-packages (from pydot->goatools) (2.3.0)\n",
      "Collecting pluggy<1.0,>=0.12\n",
      "  Downloading pluggy-0.13.1-py2.py3-none-any.whl (18 kB)\n",
      "Collecting py>=1.8.2\n",
      "  Downloading py-1.9.0-py2.py3-none-any.whl (99 kB)\n",
      "\u001b[K     |████████████████████████████████| 99 kB 1.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytest-cov\n",
      "  Downloading pytest_cov-2.10.1-py2.py3-none-any.whl (19 kB)\n",
      "Collecting coverage>=4.4\n",
      "  Downloading coverage-5.3.tar.gz (693 kB)\n",
      "\u001b[K     |████████████████████████████████| 693 kB 2.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas->goatools) (1.12.0)\n",
      "Collecting requests\n",
      "  Downloading requests-2.25.0-py2.py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 2.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "  Downloading certifi-2020.12.5-py2.py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 2.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting chardet<4,>=3.0.2\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "\u001b[K     |████████████████████████████████| 133 kB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting idna<3,>=2.5\n",
      "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/site-packages (from goatools) (1.15.4)\n",
      "Collecting statsmodels\n",
      "  Downloading statsmodels-0.12.1.tar.gz (17.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 17.4 MB 1.1 MB/s eta 0:00:01    |████████▋                       | 4.7 MB 2.5 MB/s eta 0:00:06     |██████████████████▉             | 10.3 MB 1.2 MB/s eta 0:00:06\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/site-packages (from goatools) (1.15.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/site-packages (from goatools) (1.5.4)\n",
      "Collecting patsy>=0.5\n",
      "  Downloading patsy-0.5.1-py2.py3-none-any.whl (231 kB)\n",
      "\u001b[K     |████████████████████████████████| 231 kB 184 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/site-packages (from goatools) (1.15.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas->goatools) (1.12.0)\n",
      "Collecting toml\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.2-py2.py3-none-any.whl (136 kB)\n",
      "\u001b[K     |████████████████████████████████| 136 kB 1.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wget\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "Collecting xlrd\n",
      "  Downloading xlrd-1.2.0-py2.py3-none-any.whl (103 kB)\n",
      "\u001b[K     |████████████████████████████████| 103 kB 1.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting xlsxwriter\n",
      "  Downloading XlsxWriter-1.3.7-py2.py3-none-any.whl (144 kB)\n",
      "\u001b[K     |████████████████████████████████| 144 kB 288 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting zipp>=0.5\n",
      "  Downloading zipp-3.4.0-py3-none-any.whl (5.2 kB)\n",
      "Building wheels for collected packages: goatools, docopt, coverage, statsmodels, wget\n",
      "  Building wheel for goatools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for goatools: filename=goatools-1.0.14-py3-none-any.whl size=15761526 sha256=ee272eb059bbd56eadf19f067f34fd469c49e3b6c5e5a259e14db790dce1ffb9\n",
      "  Stored in directory: /Users/hbagheri/Library/Caches/pip/wheels/52/22/d9/fc86089aeb5137f523439a19c5ca68f678ddc660a58eabeaee\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=12874 sha256=c6a848a5d93ac4652c2fecd9adf9284a781f5b7451efa23c1b74efe3036120e7\n",
      "  Stored in directory: /Users/hbagheri/Library/Caches/pip/wheels/3f/2a/fa/4d7a888e69774d5e6e855d190a8a51b357d77cc05eb1c097c9\n",
      "  Building wheel for coverage (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for coverage: filename=coverage-5.3-cp36-cp36m-macosx_10_12_x86_64.whl size=205562 sha256=d7d38c3284730af87706199dfcd3ac13f876fa474154126450c260f758392eaa\n",
      "  Stored in directory: /Users/hbagheri/Library/Caches/pip/wheels/8f/13/79/fc07c17c9c60f93002139854095de6a2e49ccbbc4858b2004d\n",
      "  Building wheel for statsmodels (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for statsmodels: filename=statsmodels-0.12.1-cp36-cp36m-macosx_10_12_x86_64.whl size=9413041 sha256=88e8f83f48198532596b8a5ee95b0608100cded93772318662077d5bc1b7ea1d\n",
      "  Stored in directory: /Users/hbagheri/Library/Caches/pip/wheels/72/b5/8a/39336c6aefc3cdd2c5e7ffc5f631fd72fd8d14c5d253223052\n",
      "  Building wheel for wget (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9688 sha256=49a537fa78fb5ed7488088e2f52cf56b8251e956f680e11917715a41c3112835\n",
      "  Stored in directory: /Users/hbagheri/Library/Caches/pip/wheels/90/1d/93/c863ee832230df5cfc25ca497b3e88e0ee3ea9e44adc46ac62\n",
      "Successfully built goatools docopt coverage statsmodels wget\n",
      "Installing collected packages: zipp, importlib-metadata, toml, py, pluggy, packaging, iniconfig, urllib3, pytest, patsy, pandas, idna, coverage, chardet, certifi, xlsxwriter, xlrd, wget, statsmodels, requests, pytest-cov, pydot, nose, docopt, goatools\n",
      "Successfully installed certifi-2020.12.5 chardet-3.0.4 coverage-5.3 docopt-0.6.2 goatools-1.0.14 idna-2.10 importlib-metadata-3.1.1 iniconfig-1.1.1 nose-1.3.7 packaging-20.7 pandas-1.1.5 patsy-0.5.1 pluggy-0.13.1 py-1.9.0 pydot-1.4.1 pytest-6.1.2 pytest-cov-2.10.1 requests-2.25.0 statsmodels-0.12.1 toml-0.10.2 urllib3-1.26.2 wget-3.2 xlrd-1.2.0 xlsxwriter-1.3.7 zipp-3.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install goatools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "../gitdata/pro_reasoned.obo: fmt(1.2) rel(61.0) 316,370 GO Terms\n",
      "Name2id dict has 315925 items.\n",
      "CPU times: user 18.9 s, sys: 11.9 s, total: 30.8 s\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from goatools.obo_parser import GODag\n",
    "godag = GODag(\"../gitdata/pro_reasoned.obo\")\n",
    "\n",
    "name2id = {}\n",
    "for item in godag.items():\n",
    "    name = item[1].name\n",
    "    name = name.replace(\",\", \"\")\n",
    "    name = name.replace(\".\", \"\")\n",
    "    name = name.lower()\n",
    "    name2id[name] = item[1].id\n",
    "    \n",
    "print(\"Name2id dict has {} items.\".format(len(name2id)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finding word embedding similarity with doc2vec\n",
    "\n",
    "Use [gensim](https://radimrehurek.com/gensim/models/doc2vec.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with mojarity vote\n",
    "def find_sim_docvec(test_data):\n",
    "    test_data = [token for token in word_tokenize(test_data.lower()) if token not in stopset and len(token) > 1]\n",
    "    item_dict = {}\n",
    "    for i in range(11):\n",
    "        most_sim_item = model.docvecs.most_similar([model.infer_vector(test_data, steps=200)])[0]\n",
    "        if most_sim_item[0] not in item_dict:\n",
    "            item_dict[most_sim_item[0]] = [0, 0.0]\n",
    "        item_dict[most_sim_item[0]][0] += 1\n",
    "        item_dict[most_sim_item[0]][1] += most_sim_item[1]\n",
    "#     print(item_dict)\n",
    "    most_sim_item = sorted(item_dict.items(), key=lambda x: x[1], reverse=True)[0]\n",
    "#     print(most_sim_item)\n",
    "    sim_doc = most_sim_item[0]\n",
    "    sim = most_sim_item[1][1] / most_sim_item[1][0]\n",
    "#     print(sim_doc, sim)\n",
    "    return sim, sim_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting progressbar2\n",
      "  Downloading progressbar2-3.53.1-py2.py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/site-packages (from progressbar2) (1.12.0)\n",
      "Collecting python-utils>=2.3.0\n",
      "  Downloading python_utils-2.4.0-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/site-packages (from progressbar2) (1.12.0)\n",
      "Installing collected packages: python-utils, progressbar2\n",
      "Successfully installed progressbar2-3.53.1 python-utils-2.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install progressbar2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183 items done. \n",
      "275900 key_ids, 56863 found, 0.20610 coverage. \n",
      "save it in \n",
      "CPU times: user 2h 21min 15s, sys: 7min 47s, total: 2h 29min 3s\n",
      "Wall time: 1h 21min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "import progressbar\n",
    "\n",
    "data_length = 1000\n",
    "# bar = progressbar.ProgressBar(maxval=data_length)\n",
    "\n",
    "sim_dict = {}\n",
    "protein_ids = {}\n",
    "\n",
    "total_num = 0\n",
    "cover_num = 0\n",
    "\n",
    "jac_sim = 0\n",
    "\n",
    "for index, names in enumerate(protein_dataset[0:data_length]):\n",
    "    pro_ids = {}\n",
    "#     bar.update(index)\n",
    "    for name in names[1:]:\n",
    "#         print(name)\n",
    "        name = name.replace(\"recname: full=\", \"\")\n",
    "        name = name.replace(\"MULTISPECIES: \".lower(), \"\")\n",
    "        if \";\" in name:\n",
    "            name = name.split(\";\")[0]\n",
    "        \n",
    "        total_num += 1\n",
    "        if name in name2id:\n",
    "            pro_id = name2id[name]\n",
    "            cover_num += 1\n",
    "            jac_sim = 1\n",
    "        else:\n",
    "            if name in sim_dict:\n",
    "                pro_id, jac_sim = sim_dict[name]\n",
    "            else:\n",
    "#                 print(name)\n",
    "                jac_sim, pro_id = find_sim_docvec(name)\n",
    "                if pro_id == \"\":\n",
    "                    continue\n",
    "                sim_dict[name] = (pro_id, jac_sim)\n",
    "        if pro_id not in pro_ids:\n",
    "            try:\n",
    "                sim_name = godag[pro_id].name.replace(\",\", \"\")\n",
    "            except:\n",
    "                sim_name = \"\"\n",
    "            try:\n",
    "                syn_name = \" ## \".join(i.replace(\",\", \"\") for i in id2synonym[pro_id])\n",
    "            except:\n",
    "                syn_name = \"\"\n",
    "            pro_ids[pro_id] = [0, name, sim_name, syn_name, jac_sim]\n",
    "#             pro_ids[pro_id] = [0, name, id2name[pro_id].replace(\",\", \"\"), jac_sim]\n",
    "        pro_ids[pro_id][0] += 1\n",
    "    protein_ids[names[0]] = pro_ids\n",
    "fp.close()\n",
    "print(\"{} items done. \".format(len(protein_ids)))\n",
    "print(\"{0} key_ids, {1} found, {2:0.5f} coverage. \".format(total_num, cover_num, cover_num / float(total_num)))\n",
    "\n",
    "print(\"save it in \" + \"\")\n",
    "# print(protein_ids)\n",
    "\n",
    "fp = open(\"../gitdata/pro_doc2vec_sim_4.csv\", \"w\")\n",
    "for key, items in protein_ids.items():\n",
    "#     fp.write(key+\"\\n\")\n",
    "    d = items.items()\n",
    "    for i in range(len(d)):\n",
    "        line_string = [key]\n",
    "        line_string.append(list(d)[i][0])\n",
    "        line_string.extend(list(d)[i][1])\n",
    "        fp.write(\",\".join([str(i) for i in line_string]) + \"\\n\")\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR:P64517\n"
     ]
    }
   ],
   "source": [
    "# test_data = \"mitochondrion inheritance\"\n",
    "test_data = \"uncharacterized protein\"\n",
    "# test_data = [token for token in word_tokenize(test_data.lower()) if token not in stopset and len(token) > 2]\n",
    "# print(test_data)\n",
    "# print(model.infer_vector(test_data, steps=200))\n",
    "# sim_doc = model.docvecs.most_similar([model.infer_vector(test_data)])\n",
    "sim, sim_doc = find_sim_docvec(test_data)\n",
    "print(sim_doc)\n",
    "# find_sim_docvec(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
